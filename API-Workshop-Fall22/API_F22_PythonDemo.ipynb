{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accessing the Twitter API with Python and Tweepy\n",
    "\n",
    "## Welcome\n",
    "\n",
    "This tutorial will provide a walkthrough of using Python and the Tweepy package to fetch Twitter data via the Twitter API.\n",
    "\n",
    "While this tutorial does not require very advanced knowledge of Python, it does assume familiarity with the basics such as creating and printing variables. It will also use the Pandas package to help with prettier output of data, but this is only for readability and you can safely follow the tutorial even without knowledge of Pandas. If you need a refresher on Python basics, we invite you to check the CCSS Python Bootcamp notebooks, which can be found at https://github.com/ccss-rs/python-bootcamp."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Technical requirements\n",
    "\n",
    "To run this tutorial, you will need two things:\n",
    "\n",
    "1. The ability to run Jupyter notebooks. This can either be done through the Binder link found on the readme (and if you're viewing this document inside the Binder web app right now, you're already good to go!), or by [installing Jupyter on your own computer](https://jupyter.org/install) and opening this file there.\n",
    "2. An approved Twitter developer account, which will let you create API keys and a bearer token which are necessary to use the API (see [this tutorial](https://cran.r-project.org/web/packages/academictwitteR/vignettes/academictwitteR-auth.html) if you have not already created a bearer token and need help doing so)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Package installation and authetication\n",
    "\n",
    "In data science, there are often some common tasks that we frequently find ourselves doing. Repeatedly writing code for these common tasks would be tiring and inconvenient. For example, when using the vanilla Twitter API, the commands can change from just a few lines to tens and tens of lines that involves things like importing a lot of libraries, more authentication, selecting end points, and managing less readable outputs (see for yourself: https://github.com/twitterdev/Twitter-API-v2-sample-code). Imagine having to do all of that every time we wanted to gather some tweets!\n",
    "\n",
    "This is where *packages* come in. Packages provide pre-written functions for common tasks.\n",
    "\n",
    "We will be using the Tweepy package to access the Twitter API in Python. Additionally, we will be using the Pandas package to do some basic formatting of the search results. The following code cell will install Tweepy and Pandas via pip."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tweepy pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the package is installed, we can *import* it, allowing us to use it in our code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step in accessing the Twitter API is *authentication*. This is the same kind of process as logging in on a website - it lets the API know who you are, so that it can check if you have permission to access the data. In tweepy, authentication is handled by a `tweepy.Client` object. To authenticate, all you need to do is tell `tweepy.Client` your *bearer token*, which is our unique ID tied to our Twitter developer account (see the [official Twitter documentation](https://developer.twitter.com/en/docs/authentication/oauth-2-0/bearer-tokens) for more details):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = tweepy.Client(\n",
    "    bearer_token=\"Paste your bearer token inside these quotes\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After running the cell above, the `client` variable now remembers our authentication info, and we can use it to talk to the Twitter API."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collecting Tweets\n",
    "\n",
    "The most common reason to use the Twitter API for social science research is searching for tweets. In tweepy, this is done using the `search_all_tweets` method. Here is a list of all the options you have for `search_all_tweets`:\n",
    "\n",
    "```\n",
    "Client.search_all_tweets(\n",
    "    query, \n",
    "    end_time, \n",
    "    expansions, \n",
    "    max_results, \n",
    "    media_fields,\n",
    "    next_token, \n",
    "    place_fields, \n",
    "    poll_fields, \n",
    "    since_id, \n",
    "    start_time, \n",
    "    tweet_fields, \n",
    "    until_id, \n",
    "    user_fields\n",
    ")\n",
    "```\n",
    "\n",
    "As you can see, we have the ability to specify our search via a query, set a date beginning or end, the number of tweets we want, and more! Let's try an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = client.search_all_tweets(query=\"Halloween\", # keyword search (in this case, we want the text \"Saint Patrick's Day\")\n",
    "                                  start_time=\"2021-10-24T00:00:00Z\", # start and end dates we want to search over\n",
    "                                  end_time=\"2021-10-31T00:00:00Z\",\n",
    "                                  max_results=100) # how many results do you want?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we have just done is ask the Twitter API for up to 100 tweets mentioning \"Halloween\" posted between October 24-31, 2021. We then saved the results to a variable called `tweets`. But what this this variable actually contain? Let's try printing it out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is not exactly very human-readable! But if we look carefully there are a few useful things we may notice. The most important thing is that inside the Response we got from the API, there is a list of tweets in a variable called `data`. In each tweet, we can see that there is an ID and the text of the tweet. This seems like the kind of data that would be best represented in a tabular format, if we want to explore it in a human-readable way. We can accomplish this using the Pandas package. Pandas provides tabular data formatting to Python in the form of R-style \"Data Frames\" (more here: https://pandas.pydata.org/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas # import the Pandas package so we can use its functions\n",
    "tweets_df = pandas.DataFrame(tweets.data) # put the data into a tabular format and save it as tweets_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(tweets_df) # more readable, right?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Much better, right?\n",
    "\n",
    "Oftentimes we may want more information than just the ID and the text. *Metadata*, the other miscellaneous descriptive properties of a tweet, can often be useful for research as well. We can specify what properties we want by using the `tweet_fields` parameter of the `get_all_tweets` method. A full list of available properties are available at the following documentation page: https://developer.twitter.com/en/docs/twitter-api/data-dictionary/object-model/tweet.\n",
    "\n",
    "As an example, let's expand our last search to give us information about who wrote each tweet, when each tweet was posted, and what conversation each tweet belongs to, in addition to the ID and text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = client.search_all_tweets(query=\"Halloween\", \n",
    "                                  start_time=\"2021-10-24T00:00:00Z\", \n",
    "                                  end_time=\"2021-10-31T00:00:00Z\",\n",
    "                                  tweet_fields=[\"id\", \"text\", \"author_id\", \"conversation_id\", \"created_at\"], # specify all desired properties in a list (ID and text must be explicitly included if you still want them)\n",
    "                                  max_results=100) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(pandas.DataFrame(tweets.data)) # just like last time, use Pandas to format the output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count Tweets\n",
    "\n",
    "So what if you didn't want to spend all of your limit on collecting tweets hoping the sample was what you wanted? Or, you wanted to get an estimate of how many tweets would be pulled for a particular query?\n",
    "\n",
    "This is where tweet counts come in.\n",
    "\n",
    "Tweet counts don't count toward your tweet cap, but they can reveal really important information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = client.get_all_tweets_count(\n",
    "    query=\"#vote\", # let's try a hashtag analysis\n",
    "    start_time=\"2020-10-15T00:00:00Z\",\n",
    "    end_time=\"2020-11-15T00:00:00Z\",\n",
    "    granularity=\"day\" # tell the API to count number of tweets per day (can also do hour or minute).\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(pandas.DataFrame(count.data)) # just like last time, use Pandas to format the output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These results show the number of tweets by the level of granularity you selected. From this information we can now narrow our search to specific days or use this to create a visualization of tweets for a particular event.\n",
    "\n",
    "From these results, we can see how something as simple as a tweet count can already reveal interesting phenomena. In this case, the number of tweets per day mentioning the hashtag \\#vote seems to rise starting around the end of October 2020, peaking between November 3rd and 4th and then dropping sharply after that. Why do you think this is?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User Tweets\n",
    "\n",
    "Sometimes we may be interested in finding tweets by a specific user. We have to be careful here however! As humans, we typically identify users by their username, aka Twitter handle (e.g., @InfoTisayentist for Aspen). But the Twitter API instead identifies users by a numerical ID. Thankfully, Tweepy gives us tools to convert between the two:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_info = client.get_user(username=\"pete_enns\") # look up information for the user @pete_enns\n",
    "print(user_info) # note the \"id\" variable!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have found a user's numerical ID, we can use this to look up their tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "profile_tweets = client.get_users_tweets(\n",
    "    id=3229291352, # @pete_enns' ID that we retrieved above\n",
    "    start_time=\"2020-10-15T00:00:00Z\",\n",
    "    end_time=\"2020-10-22T00:00:00Z\",\n",
    "    max_results=100\n",
    ")\n",
    "display(pandas.DataFrame(profile_tweets.data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conversation Threads\n",
    "\n",
    "Here we can see how using one query can factor into other searches. You could use a tweet or user search to identify a specific conversation that you want to dig into. The key to this is the `query` parameter in `search_all_tweets`. Previously, we have only provided text queries. But we can also query by other fields, as we will demonstrate here with `conversation_id`:\n",
    "\n",
    "Feel free to swap the conversation ID with something more interesting!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thread_tweets = client.search_all_tweets(\n",
    "    # Replace with ID of your choice to get replies (this ID is from the Halloween original set)\n",
    "    query=\"conversation_id:1454363540245069827\",\n",
    "    start_time = \"2021-10-24T00:00:00Z\",\n",
    "    end_time = \"2021-10-31T00:00:00Z\",\n",
    "    max_results=100\n",
    ")\n",
    "display(pandas.DataFrame(thread_tweets.data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Geo-tagged Tweets\n",
    "\n",
    "Text queries and field-based queries are not mutually exclusive, a single query can contain both plain text to search for and specify individual fields. Let's demonstrate using a common use case: location filtering.\n",
    "\n",
    "Location is really important information. Remember when Facebook added the \"checking in\" feature during natural disasters? Well, we can search for Tweets that specifically have location information embedded in those tweets.\n",
    "\n",
    "Specifying `has:geo` in a query filters the search criteria to only the tweets with this information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vote_geo = client.search_all_tweets(\n",
    "    # what if we want to find #vote tweets from specific cities/counties? maybe they are geotagged...\n",
    "    query=\"#vote has:geo\", \n",
    "    start_time=\"2020-11-03T00:00:00Z\",\n",
    "    end_time=\"2020-11-04T00:00:00Z\",\n",
    "    max_results=20\n",
    ")\n",
    "display(pandas.DataFrame(vote_geo.data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Location information can be `geo.place_id` or coordinates. If the former, then one more step is needed to convert the place code to a location (on how to do that here: https://developer.twitter.com/en/docs/twitter-api/v1/geo/places-near-location/api-reference/get-geo-reverse_geocode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Parameters\n",
    "\n",
    "So far, we've augmented our naive text searches with two advanced parameters: `conversation_id` and `has:geo`. But this doesn't even begin to scratch the surface of ways we can customize our queries! There is too much to cover in a single workshop, but we encourage you to read the official documentation to learn more about all the parameters you can use: https://developer.twitter.com/en/docs/twitter-api/tweets/search/integrate/build-a-query. For now, we will leave you with an example of a specific query we might use in practice:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_advanced = client.search_all_tweets(\n",
    "    # an advanced query combining several parameters\n",
    "    query='#vote has:geo place:\"ithaca\" lang:en -is:retweet -is:verified',\n",
    "    start_time=\"2020-11-03T00:00:00Z\",\n",
    "    end_time=\"2020-11-04T00:00:00Z\",\n",
    "    max_results=20\n",
    ")\n",
    "display(pandas.DataFrame(tweets_advanced.data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Word\n",
    "\n",
    "Congratulations! You have overcome perhaps the most daunting hurdle in using APIs. Which would be using an API. It is important to remember that your queries will not always be perfect or capture exactly what you want. That's okay, there are a slew of resources on data management and cleaning that help you get to the final step which will look more like a dataset worthy of analysis.\n",
    "\n",
    "Well done and I wish you the best with your computational social science endeavors!\n",
    "\n",
    "## Resources\n",
    "Official Twitter API V2 developer documentation: https://developer.twitter.com/en/docs/twitter-api\n",
    "\n",
    "Twitter API V2 Sample Code: https://github.com/twitterdev/getting-started-with-the-twitter-api-v2-for-academic-research "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
